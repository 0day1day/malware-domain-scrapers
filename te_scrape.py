'''te_scrape.py

This script scrapes ThreatExpert using Beautiful Soup 4

IT RETURNS THE THREAT EXPERT REPORT PAGES NOT THE MALDOMAINS... YET 

Python 2.7 docs can be found here: http://docs.python.org/2.7/
urllib2 docs can be found here: http://docs.python.org/2.7/library/urllib2.html
bs4 docs can be found here: http://crummy.com/software/BeautifulSoup/bs4/doc/

'''

from bs4 import BeautifulSoup
import re
import urllib2
import argparse
import sys
import csv


# Set up the script's argument parser
parser = argparse.ArgumentParser(description='Scrapes the Threat Expert for malicious domain names.', usage='te_scraper.py [-h] [-v] [-b] [-o <file>] [-c <file>]')
parser.add_argument('-o', '--output', help='specify a file to write to')
parser.add_argument('-c', '--csv', help='specify a csv to write to')
parser.add_argument('-v', '--verbose', action='store_true', help='be verbose')
parser.add_argument('-b', '--bad', action='store_true', help='scrape the known bad report table')

# Parse through the arguments given by the user (if any) and store them in args
args = parser.parse_args()

# Set up HTTP request
if args.bad:
  request = urllib2.Request('http://www.threatexpert.com/reports.aspx?sl=1')
else:
  request = urllib2.Request('http://www.threatexpert.com/reports.aspx')
  
request.add_header('User-Agent', 'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:18.0) Gecko/20100101 Firefox/18.0')

# Get the HTML from TE
html = urllib2.urlopen(request).read()

# Create the Beautiful Soup object
soup = BeautifulSoup(html)

# Find all of the tables on the TE page (there are 9) and put them in a list
tables = soup.findAll('table')

# We want the 8th table which is 7 in the list.
table = tables[7]

# Isolate the table rows, <tr>, and add them to another list.
rows = table.findAll(['tr'])

# For each entry in the rows list
reports = []
for row in rows:
  
  # Grab all of the <td> entries out of each row
  # Save the fourth column (Findings) of each row into list
  third_column = row.findAll('td')[3]
  
  # Keep only the <a> entries from the fourth row thrid column
  for a_tag in third_column.findAll('a'):

    # Grab the href attribute and prepend the domain name.
    href = a_tag.get('href')
    link = 'http://www.threatexpert.com/' + href
    reports.append(str(link))

# Now we have each report link on the first page of TE
# WE NEED TO SCRAPE DEEPER
# We must scrape each of these pages and see if there is web traffic

# Create an empty list to put our web traffic reports in
web_reports = []
urls = []
# For each report in the reports list
#  get the html from the report
#  Create a new Beautiful Soup object with the new html
#  Find all the <li> entries in the html and if the contain URL or Internet
#  Connection add them to our new web_reports list
for report in reports:
  report_html = urllib2.urlopen(urllib2.Request(report)).read()
  rsoup = BeautifulSoup(report_html)
  urls.append(rsoup.find_all(text=re.compile(r'.{3,}[a-zA-Z0-9-\.]\.{2,4}[a-zA-Z]')))

  if (rsoup.find_all(name='li', text=re.compile(r'URL'))) or \
     (rsoup.find_all(name='li', text=re.compile(r'Internet\ Connection'))) \
     != []:
    web_reports.append(report)

# Print the contents of the list (if any)
if len(web_reports) != 0:
  if args.csv:
    csv_file = open(args.csv, 'wb')
    writer = csv.writer(csv_file)
    writer.writerow(web_reports)
    csv_file.close()
  elif args.output:
    f = open(args.output, 'wb')
    for r in web_reports:
      f.write('%s \n' %(r))
    f.close()
  else:
    print urls
    for r in web_reports:
      print r
else:
  print '[!] No reports found with URL data.'
